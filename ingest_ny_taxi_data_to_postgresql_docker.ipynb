{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "772096bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from time import time\n",
    "df = pd.read_csv(\"yellow_tripdata_2019-01.csv\", \\\n",
    "                 parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'],\\\n",
    "                 nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a22e2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" INTEGER,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" INTEGER,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df,\"yellow_taxi_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1314f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "# create_engine(postgresql://user:passwpord@localhost:port/database_name)\n",
    "# pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "90dd48d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x138eeccd0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the connection\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f4b09ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = 'yellow_taxi_data'\n",
    "# get the postgresql shcema\n",
    "print(pd.io.sql.get_schema(df,table_name,con=engine,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c69a417c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# definition in postgresql dialect\n",
    "\n",
    "table_name = 'yellow_tripdata_2019-01.csv'\n",
    "# get the postgresql shcema\n",
    "schema = pd.io.sql.get_schema(df,table_name,con=engine)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "#     drop table if exists\n",
    "    conn.execute(f'DROP TABLE IF EXISTS {table_name}')\n",
    "#     create the table\n",
    "    conn.execute(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871eaec",
   "metadata": {},
   "source": [
    "or use \n",
    "\n",
    "```sql\n",
    "df.head(0).to_sql(name = table_name,\\\n",
    "                 con= engine,\n",
    "                 if_exists='replace')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a4a15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = pd.read_csv(\"yellow_tripdata_2019-01.csv\", \\\n",
    "             parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'],\\\n",
    "#              iterator=True,\\\n",
    "             chunksize=10000)\n",
    "type(datda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83dea217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100000 chunk data ... took 30.089\n",
      "Inserted 100000 chunk data ... took 25.849\n",
      "Inserted 100000 chunk data ... took 30.159\n",
      "Inserted 100000 chunk data ... took 24.380\n",
      "Inserted 100000 chunk data ... took 29.061\n",
      "Inserted 100000 chunk data ... took 27.801\n",
      "Inserted 100000 chunk data ... took 37.121\n",
      "Inserted 100000 chunk data ... took 34.737\n",
      "Inserted 100000 chunk data ... took 25.866\n",
      "Inserted 100000 chunk data ... took 32.040\n",
      "Inserted 100000 chunk data ... took 35.872\n",
      "Inserted 100000 chunk data ... took 25.430\n",
      "Inserted 100000 chunk data ... took 23.279\n",
      "Inserted 100000 chunk data ... took 27.261\n",
      "Inserted 100000 chunk data ... took 40.142\n",
      "Inserted 100000 chunk data ... took 27.178\n",
      "Inserted 100000 chunk data ... took 25.193\n",
      "Inserted 100000 chunk data ... took 28.932\n",
      "Inserted 100000 chunk data ... took 36.720\n",
      "Inserted 100000 chunk data ... took 30.067\n",
      "Inserted 100000 chunk data ... took 28.263\n",
      "Inserted 100000 chunk data ... took 32.118\n",
      "Inserted 100000 chunk data ... took 31.204\n",
      "Inserted 100000 chunk data ... took 32.336\n",
      "Inserted 100000 chunk data ... took 27.573\n",
      "Inserted 100000 chunk data ... took 33.184\n",
      "Inserted 100000 chunk data ... took 32.283\n",
      "Inserted 100000 chunk data ... took 27.441\n",
      "Inserted 100000 chunk data ... took 24.460\n",
      "Inserted 100000 chunk data ... took 29.785\n",
      "Inserted 100000 chunk data ... took 51.838\n",
      "Inserted 100000 chunk data ... took 29.224\n",
      "Inserted 100000 chunk data ... took 25.537\n",
      "Inserted 100000 chunk data ... took 24.402\n",
      "Inserted 100000 chunk data ... took 25.730\n",
      "Inserted 100000 chunk data ... took 31.205\n",
      "Inserted 100000 chunk data ... took 35.580\n",
      "Inserted 100000 chunk data ... took 25.386\n",
      "Inserted 100000 chunk data ... took 26.881\n",
      "Inserted 100000 chunk data ... took 35.244\n",
      "Inserted 100000 chunk data ... took 54.965\n",
      "Inserted 100000 chunk data ... took 32.883\n",
      "Inserted 100000 chunk data ... took 45.062\n",
      "Inserted 100000 chunk data ... took 30.783\n",
      "Inserted 100000 chunk data ... took 37.280\n",
      "Inserted 100000 chunk data ... took 52.829\n",
      "Inserted 100000 chunk data ... took 36.649\n",
      "Inserted 100000 chunk data ... took 26.697\n",
      "Inserted 100000 chunk data ... took 40.452\n",
      "Inserted 100000 chunk data ... took 36.185\n",
      "Inserted 100000 chunk data ... took 30.101\n",
      "Inserted 100000 chunk data ... took 36.446\n",
      "Inserted 100000 chunk data ... took 32.543\n",
      "Inserted 100000 chunk data ... took 48.103\n",
      "Inserted 100000 chunk data ... took 123.191\n",
      "Inserted 100000 chunk data ... took 95.397\n",
      "Inserted 100000 chunk data ... took 88.095\n",
      "Inserted 100000 chunk data ... took 96.653\n",
      "Inserted 100000 chunk data ... took 45.581\n",
      "Inserted 100000 chunk data ... took 33.677\n",
      "Inserted 100000 chunk data ... took 27.850\n",
      "Inserted 100000 chunk data ... took 79.060\n",
      "Inserted 100000 chunk data ... took 32.570\n",
      "Inserted 100000 chunk data ... took 36.880\n",
      "Inserted 100000 chunk data ... took 33.013\n",
      "Inserted 100000 chunk data ... took 54.294\n",
      "Inserted 100000 chunk data ... took 28.892\n",
      "Inserted 100000 chunk data ... took 32.380\n",
      "Inserted 100000 chunk data ... took 26.707\n",
      "Inserted 100000 chunk data ... took 32.551\n",
      "Inserted 100000 chunk data ... took 29.646\n",
      "Inserted 100000 chunk data ... took 38.705\n",
      "Inserted 100000 chunk data ... took 70.680\n",
      "Inserted 100000 chunk data ... took 78.942\n",
      "Inserted 100000 chunk data ... took 58.859\n",
      "Inserted 100000 chunk data ... took 80.092\n",
      "Inserted 67792 chunk data ... took 39.565\n"
     ]
    }
   ],
   "source": [
    "# read the data in chunks\n",
    "data_iter = pd.read_csv(\"yellow_tripdata_2019-01.csv\", \\\n",
    "                         parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'],\\\n",
    "                         chunksize=100000)\n",
    "\n",
    "table_name = 'yellow_taxi_data'\n",
    "\n",
    "# insert the in chunks\n",
    "for data in data_iter:\n",
    "    start_time = time()\n",
    "    data.to_sql(name= table_name, con=engine, if_exists= 'append')\n",
    "    print(f'Inserted {len(data)} chunk data ... took %.3f'%(time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896a4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
