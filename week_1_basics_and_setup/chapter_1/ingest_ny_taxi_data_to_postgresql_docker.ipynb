{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Draft implementation of data ingestion from  url to local host pgadmin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772096bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from time import time\n",
    "\n",
    "# load data from csv file into a dataframe\n",
    "df = pd.read_csv(\"yellow_tripdata_2019_01.csv\", \\\n",
    "                 parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'],\\\n",
    "                 nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22e2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" INTEGER,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" INTEGER,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create data schema for creating the data on postgresql\n",
    "print(pd.io.sql.get_schema(df,\"yellow_taxi_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1314f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batabase connection to postgresql in docker, hence keep docker postgress running\n",
    "engine = create_engine('postgresql://root:root@localhost:5431/ny_taxi')\n",
    "# create_engine(postgresql://user:password@host:port/database_name)\n",
    "# NB: pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90dd48d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7f97e16cb4f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the connection\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4b09ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = 'yellow_taxi_data'\n",
    "# get the postgresql shcema\n",
    "print(pd.io.sql.get_schema(df,table_name,con=engine,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a417c",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "```python\n",
    "# definition in postgresql dialect\n",
    "table_name = 'yellow_tripdata_2019_01.csv'\n",
    "# get the postgresql shcema\n",
    "schema = pd.io.sql.get_schema(df,table_name,con=engine)\n",
    "with engine.connect() as conn:\n",
    "#     drop table if exists\n",
    "    # conn.execute(text(f'DROP TABLE IF EXISTS {table_name}'))\n",
    "#     create the table\n",
    "    conn.execute(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'yellow_taxi_data'\n",
    "df.head(0).to_sql(name = table_name, con= engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4a15b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.parsers.readers.TextFileReader"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter: pd.io.parsers.readers.TextFileReader =    pd.read_csv(\"yellow_tripdata_2019_01.csv\", \\\n",
    "                               parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'],\\\n",
    "                               iterator=True,\\\n",
    "                               chunksize=10000)\n",
    "type(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83dea217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100000 chunk data ... took 8.285\n",
      "Inserted 100000 chunk data ... took 7.765\n",
      "Inserted 100000 chunk data ... took 7.709\n",
      "Inserted 100000 chunk data ... took 7.885\n",
      "Inserted 100000 chunk data ... took 7.756\n",
      "Inserted 100000 chunk data ... took 7.862\n",
      "Inserted 100000 chunk data ... took 7.780\n",
      "Inserted 100000 chunk data ... took 7.879\n",
      "Inserted 100000 chunk data ... took 7.873\n",
      "Inserted 100000 chunk data ... took 7.949\n",
      "Inserted 100000 chunk data ... took 7.749\n",
      "Inserted 100000 chunk data ... took 7.826\n",
      "Inserted 100000 chunk data ... took 7.928\n",
      "Inserted 100000 chunk data ... took 7.837\n",
      "Inserted 100000 chunk data ... took 8.178\n",
      "Inserted 100000 chunk data ... took 7.831\n",
      "Inserted 100000 chunk data ... took 7.840\n",
      "Inserted 100000 chunk data ... took 8.252\n",
      "Inserted 100000 chunk data ... took 7.958\n",
      "Inserted 100000 chunk data ... took 7.779\n",
      "Inserted 100000 chunk data ... took 7.604\n",
      "Inserted 100000 chunk data ... took 7.689\n",
      "Inserted 100000 chunk data ... took 7.779\n",
      "Inserted 100000 chunk data ... took 7.723\n",
      "Inserted 100000 chunk data ... took 7.913\n",
      "Inserted 100000 chunk data ... took 7.728\n",
      "Inserted 100000 chunk data ... took 7.608\n",
      "Inserted 100000 chunk data ... took 7.713\n",
      "Inserted 100000 chunk data ... took 7.791\n",
      "Inserted 100000 chunk data ... took 7.779\n",
      "Inserted 100000 chunk data ... took 7.664\n",
      "Inserted 100000 chunk data ... took 8.044\n",
      "Inserted 100000 chunk data ... took 7.979\n",
      "Inserted 100000 chunk data ... took 7.623\n",
      "Inserted 100000 chunk data ... took 7.730\n",
      "Inserted 100000 chunk data ... took 7.639\n",
      "Inserted 100000 chunk data ... took 7.768\n",
      "Inserted 100000 chunk data ... took 7.710\n",
      "Inserted 100000 chunk data ... took 7.796\n",
      "Inserted 100000 chunk data ... took 7.884\n",
      "Inserted 100000 chunk data ... took 7.827\n",
      "Inserted 100000 chunk data ... took 7.728\n",
      "Inserted 100000 chunk data ... took 7.698\n",
      "Inserted 100000 chunk data ... took 7.722\n",
      "Inserted 100000 chunk data ... took 7.613\n",
      "Inserted 100000 chunk data ... took 7.617\n",
      "Inserted 100000 chunk data ... took 7.621\n",
      "Inserted 100000 chunk data ... took 7.982\n",
      "Inserted 100000 chunk data ... took 7.814\n",
      "Inserted 100000 chunk data ... took 7.872\n",
      "Inserted 100000 chunk data ... took 7.775\n",
      "Inserted 100000 chunk data ... took 7.967\n",
      "Inserted 100000 chunk data ... took 7.786\n",
      "Inserted 100000 chunk data ... took 7.900\n",
      "Inserted 100000 chunk data ... took 7.941\n",
      "Inserted 100000 chunk data ... took 8.121\n",
      "Inserted 100000 chunk data ... took 8.169\n",
      "Inserted 100000 chunk data ... took 7.919\n",
      "Inserted 100000 chunk data ... took 7.795\n",
      "Inserted 100000 chunk data ... took 7.947\n",
      "Inserted 100000 chunk data ... took 7.806\n",
      "Inserted 100000 chunk data ... took 7.955\n",
      "Inserted 100000 chunk data ... took 7.904\n",
      "Inserted 100000 chunk data ... took 8.059\n",
      "Inserted 100000 chunk data ... took 7.942\n",
      "Inserted 100000 chunk data ... took 8.005\n",
      "Inserted 100000 chunk data ... took 8.054\n",
      "Inserted 100000 chunk data ... took 8.028\n",
      "Inserted 100000 chunk data ... took 7.959\n",
      "Inserted 100000 chunk data ... took 7.956\n",
      "Inserted 100000 chunk data ... took 7.876\n",
      "Inserted 100000 chunk data ... took 8.246\n",
      "Inserted 100000 chunk data ... took 7.822\n",
      "Inserted 100000 chunk data ... took 7.752\n",
      "Inserted 100000 chunk data ... took 7.911\n",
      "Inserted 100000 chunk data ... took 7.744\n",
      "Inserted 67792 chunk data ... took 5.310\n"
     ]
    }
   ],
   "source": [
    "# read the data in chunks\n",
    "data_iter: pd.io.parsers.readers.TextFileReader =  pd.read_csv(\"yellow_tripdata_2019_01.csv\", \\\n",
    "                                parse_dates=['tpep_pickup_datetime','tpep_dropoff_datetime'],\\\n",
    "                                chunksize=100000)\n",
    "\n",
    "# insert the in chunks\n",
    "for data in data_iter:\n",
    "    start_time = time()\n",
    "    data.to_sql(name= table_name, con=engine, if_exists= 'append')\n",
    "    print(f'Inserted {len(data)} chunk data ... took %.3f'%(time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896a4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
